## üß™ Lista de **30 Exerc√≠cios** para Treinar PySpark

### üõ†Ô∏è **B√°sico a Intermedi√°rio**

1. Ler todos os arquivos CSV usando `spark.read.csv` e inferir schema.
2. Verificar os tipos de dados de cada DataFrame e corrigir se necess√°rio.
3. Quantos usu√°rios √∫nicos h√° na base?
4. Qual √© o valor total de vendas por categoria de produto?
5. Quais os 10 produtos mais vendidos em quantidade?
6. M√©dia de pre√ßo dos produtos por categoria.
7. Agrupe os pedidos por status e calcule a propor√ß√£o de cada um.
8. Quantos pedidos cada vendedor recebeu?
9. Quais s√£o os 10 usu√°rios que mais compraram em valor?
10. Liste os usu√°rios com mais de 5 avalia√ß√µes feitas.
11. Quais estados t√™m mais usu√°rios cadastrados?
12. Quais as formas de pagamento mais usadas?
13. Encontre o tempo m√©dio entre a data do pedido e a avalia√ß√£o.
14. Qual √© a nota m√©dia dos produtos por categoria?
15. Liste os produtos com nota m√©dia maior que 4.5.

### üîÑ **Transforma√ß√µes Avan√ßadas**

16. Crie uma coluna de receita por item (`price * quantity`).
17. Calcule a receita total por vendedor.
18. Junte produtos e vendedores para exibir os nomes das empresas com seus produtos.
19. Fa√ßa o join dos pedidos com os pagamentos e analise discrep√¢ncias de valor.
20. Use `window functions` para rankear os produtos mais vendidos por categoria.
21. Calcule a m√©dia m√≥vel de vendas por semana.
22. Encontre sess√µes com dura√ß√£o maior que 60 minutos.
23. Descubra o tempo total gasto na plataforma por usu√°rio.
24. Liste os usu√°rios que compraram produtos de mais de 3 categorias distintas.

### üß† **An√°lise e Machine Learning**

25. Crie buckets de valor de compra (baixo, m√©dio, alto) e agrupe usu√°rios.
26. Realize uma clusteriza√ß√£o de usu√°rios com KMeans usando gasto total e n√∫mero de pedidos.
27. Classifique usu√°rios em ‚Äúativos‚Äù, ‚Äúregulares‚Äù e ‚Äúinativos‚Äù com base em sess√µes e pedidos.
28. Detecte outliers de pagamentos usando desvio padr√£o.
29. Crie uma coluna "churn" (1 se o usu√°rio n√£o compra h√° 6 meses) e use para an√°lise.
30. Modele a propens√£o de um usu√°rio avaliar um pedido com base em dados hist√≥ricos (classifica√ß√£o).


### üß† **Analytics, UDFs e Views**

31. Crie uma view tempor√°ria com os produtos e a receita total gerada por eles.
32. Usando `SQL`, selecione os 5 vendedores que mais faturaram.
33. Crie uma coluna categ√≥rica "tipo\_usuario" com `UDF` baseada no n√∫mero de pedidos: "novo", "m√©dio", "veterano".
34. Use `broadcast join` entre products e sellers para otimizar uma agrega√ß√£o.
35. Filtre as avalia√ß√µes onde o review foi escrito mais de 7 dias ap√≥s o pedido.
36. Agrupe os usu√°rios por estado e calcule o ticket m√©dio por estado.
37. Analise o percentual de usu√°rios que fizeram ao menos 1 pedido nos √∫ltimos 30 dias.
38. Crie uma coluna com o tempo m√©dio entre compras de cada usu√°rio.
39. Calcule a taxa de convers√£o de sess√µes em pedidos por usu√°rio.
40. Determine a sazonalidade mensal de vendas (agrupe por m√™s e ano).

### üîÑ **Manipula√ß√£o de Dados Complexos**

41. Use `explode` para simular listas de interesses do usu√°rio (mock) e agregue por interesse.
42. Identifique o produto mais bem avaliado em cada categoria com `dense_rank`.
43. Crie um DataFrame com a sequ√™ncia temporal de pedidos por usu√°rio.
44. Calcule o tempo m√©dio de entrega considerando diferen√ßa entre `order_date` e `review_date`.
45. Crie uma coluna "tipo\_pagamento\_seguro" com um `when/otherwise` para classificar tipos.
46. Converta os campos de datas para `date` e agrupe os dados por trimestre.
47. Compare o n√∫mero de produtos com avalia√ß√£o acima de 4 entre os vendedores.
48. Filtre produtos que nunca foram vendidos.
49. Agrupe usu√°rios que fizeram compras com 3 ou mais m√©todos de pagamento distintos.
50. Calcule o Lifetime Value (LTV) de cada usu√°rio.

### üß¨ **Performance e Particionamento**

51. Reparticione os dados de pedidos por status para performance.
52. Escreva os produtos em Parquet particionando por categoria.
53. Crie um cache dos dados de usu√°rios mais ativos.
54. Conte o n√∫mero de joins realizados com broadcast e sem broadcast.
55. Otimize a leitura de arquivos parquet com `pushDownPredicate`.

### ü§ñ **MLlib: Machine Learning em Spark**

56. Use `StringIndexer` e `VectorAssembler` para preparar dados de usu√°rios.
57. Treine um modelo de classifica√ß√£o para prever se um usu√°rio vai avaliar um produto.
58. Use `KMeans` para clusterizar vendedores baseado em volume e receita.
59. Avalie a acur√°cia de um modelo de classifica√ß√£o com `MulticlassClassificationEvaluator`.
60. Crie um pipeline completo com transforma√ß√£o + modelo + avalia√ß√£o.